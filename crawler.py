import os
import shutil
import tempfile
import subprocess
import glob
from database import db
from ai_service import ai

class RepoCrawler:
    def __init__(self):
        self.is_busy = False
        self.logs = [] 
        self.temp_repo_path = None 
        self.found_files = []

    def add_log(self, msg):
        print(f"[Crawler] {msg}")
        self.logs.append(msg)

    def scan_structure(self, repo_url):
        """Step 1: ä»…ä¸‹è½½ä»£ç ï¼Œåˆ—å‡ºæ–‡ä»¶"""
        if self.is_busy: return
        self.is_busy = True
        self.logs = []
        self.found_files = []
        
        # æ¸…ç†æ—§æ•°æ®
        if self.temp_repo_path and os.path.exists(self.temp_repo_path):
            shutil.rmtree(self.temp_repo_path, ignore_errors=True)

        self.temp_repo_path = tempfile.mkdtemp()
        
        try:
            self.add_log(f"æ­£åœ¨è¿æ¥ä»“åº“: {repo_url}")
            subprocess.run(["git", "clone", "--depth", "1", repo_url, self.temp_repo_path], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # æ‰«æ py æ–‡ä»¶
            all_py_files = glob.glob(os.path.join(self.temp_repo_path, "**/*.py"), recursive=True)
            
            for f_path in all_py_files:
                if "__init__.py" in f_path or "setup.py" in f_path: continue
                # è¿‡æ»¤è¿‡å°æˆ–è¿‡å¤§çš„æ–‡ä»¶
                if os.path.getsize(f_path) < 20 or os.path.getsize(f_path) > 30000: continue
                
                rel_path = os.path.relpath(f_path, self.temp_repo_path)
                self.found_files.append(rel_path)
            
            self.add_log(f"âœ… æ‰«æå®Œæˆ! å‘ç° {len(self.found_files)} ä¸ªæ–‡ä»¶ã€‚è¯·é€‰æ‹©éœ€è¦å¯¼å…¥çš„æ–‡ä»¶ã€‚")
            
        except Exception as e:
            self.add_log(f"âŒ æ‰«æå‡ºé”™: {str(e)}")
            self.temp_repo_path = None
        finally:
            self.is_busy = False

    def process_selected(self, selected_indices):
        """Step 2: å¯¹é€‰ä¸­çš„æ–‡ä»¶è¿›è¡Œ AI åˆ†æ"""
        if self.is_busy: return
        self.is_busy = True
        
        total = len(selected_indices)
        self.add_log(f"å¼€å§‹ AI åˆ†æ {total} ä¸ªæ–‡ä»¶...")
        
        try:
            success_count = 0
            for i, idx in enumerate(selected_indices):
                if idx < 0 or idx >= len(self.found_files): continue
                
                rel_path = self.found_files[idx]
                full_path = os.path.join(self.temp_repo_path, rel_path)
                
                self.add_log(f"[{i+1}/{total}] åˆ†æä¸­: {rel_path}")
                
                try:
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                        code = f.read()
                    
                    meta = ai.generate_problem_metadata(code)
                    
                    if meta and isinstance(meta, dict):
                        # æ•°æ®æ¸…æ´—
                        if isinstance(meta.get('knowledge'), list):
                            meta['knowledge'] = ", ".join(str(x) for x in meta['knowledge'])
                        if not meta.get('knowledge'): meta['knowledge'] = "ç»¼åˆ"
                        
                        meta['source_repo'] = "Git Import"
                        meta['file_path'] = rel_path
                        meta['code'] = code
                        
                        # 1. ä¿å­˜é¢˜ç›®ä¸»ä½“ (è¿™éƒ¨åˆ†é€»è¾‘å¾®è°ƒï¼Œè·å–æ’å…¥åçš„ ID)
                        # æ³¨æ„ï¼šä½ éœ€è¦ä¿®æ”¹ add_problem_from_crawler è®©å®ƒè¿”å› IDï¼Œæˆ–è€…å…ˆæŸ¥ ID
                        db.add_problem_from_crawler(meta)
                        
                        # è·å–åˆšåˆšæ’å…¥çš„é¢˜ç›® ID
                        conn = db.get_conn()
                        cursor = conn.cursor()
                        cursor.execute("SELECT id FROM problems WHERE source_repo=? AND file_path=?", (meta['source_repo'], meta['file_path']))
                        pid = cursor.fetchone()[0]
                        conn.close()

                        # 2. ã€æ–°å¢ã€‘ä¿å­˜å¤šç»„æµ‹è¯•ç”¨ä¾‹
                        # å…ˆæ¸…ç©ºæ—§çš„ï¼ˆé˜²æ­¢é‡å¤å¯¼å…¥æ—¶å †ç§¯ï¼‰
                        db.clear_test_cases(pid)
                        
                        # å¦‚æœ AI ç”Ÿæˆäº† test_cases åˆ—è¡¨
                        if 'test_cases' in meta and isinstance(meta['test_cases'], list):
                            for case in meta['test_cases']:
                                # ç¡®ä¿è¾“å…¥æ•°æ®æœ€åæœ‰æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ EOFError
                                inp = case.get('input', '')
                                out = case.get('output', '')
                                
                                # æŠ€å·§ï¼šå¤„ç†å¤šè¡Œè¾“å…¥ã€‚
                                # å¦‚æœç¨‹åºæœ‰å¤šä¸ª input()ï¼Œæ•°æ®åº“å­˜çš„æ•°æ®å¿…é¡»æ˜¯ "Line1\nLine2"
                                # è¿™é‡Œçš„ inp åº”è¯¥æ˜¯ AI ç”Ÿæˆå¥½çš„å¸¦ \n çš„å­—ç¬¦ä¸²
                                
                                db.add_test_case(pid, inp, out)
                                
                            print(f"âœ… å·²ä¿å­˜ {len(meta['test_cases'])} ç»„æµ‹è¯•æ•°æ®")
                        else:
                            # å…¼å®¹æ—§é€»è¾‘ï¼šå¦‚æœ AI æ²¡ç”Ÿæˆæ•°ç»„ï¼Œç”¨å•ç»„æ•°æ®å…œåº•
                            db.add_test_case(pid, meta.get('input', ''), meta.get('output', ''))

                        success_count += 1
                    else:
                        self.add_log(f"âš ï¸ è·³è¿‡ {rel_path}: AI æ•°æ®ç”Ÿæˆå¤±è´¥")
                        
                except Exception as e:
                    print(f"File Error: {e}")
            
            self.add_log(f"ğŸ‰ å…¨éƒ¨å®Œæˆ! æˆåŠŸå…¥åº“: {success_count} é¢˜ã€‚")
            
        except Exception as e:
            self.add_log(f"âŒ æµç¨‹ä¸­æ–­: {e}")
        finally:
            # å®Œæˆåæ¸…ç†
            if self.temp_repo_path and os.path.exists(self.temp_repo_path):
                shutil.rmtree(self.temp_repo_path, ignore_errors=True)
                self.temp_repo_path = None
                self.found_files = []
            self.is_busy = False

    def organize_database(self):
        self.is_busy = True
        self.logs.append("å¼€å§‹æ•´ç†çŸ¥è¯†ç‚¹...")
        try:
            all_probs = db.get_all_problems()
            summary = [{"id": p["id"], "title": p["title"]} for p in all_probs]
            updates = ai.cluster_problems(summary)
            if updates:
                db.update_knowledge_tags(updates)
                self.logs.append(f"âœ… æ•´ç†å®Œæˆï¼Œæ›´æ–° {len(updates)} æ¡ã€‚")
        finally:
            self.is_busy = False

crawler_service = RepoCrawler()